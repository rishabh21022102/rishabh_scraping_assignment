{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabh21022102/rishabh_scraping_assignment/blob/main/Rishabh_scraping_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ4OivM8evC_"
      },
      "source": [
        "### Validation Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xS3QDBVyfQN"
      },
      "outputs": [],
      "source": [
        "class Validation:\n",
        "    @staticmethod\n",
        "    def validate_price(sale_price, original_price):\n",
        "        \"\"\"Validate that sale price is less than or equal to original price.\"\"\"\n",
        "        if sale_price > original_price:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_mandatory_fields(title):\n",
        "        \"\"\"Validate that title, product_id, and model_id are mandatory fields.\"\"\"\n",
        "        if not title:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_variants(variants_data):\n",
        "        \"\"\"Validate each variant has images and their respective prices.\"\"\"\n",
        "        for variant in variants_data:\n",
        "            if 'images' not in variant or 'prices' not in variant:\n",
        "                return False\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIgCvjaFe1pi"
      },
      "source": [
        "### https://www.traderjoes.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsGyE-vnzIPt",
        "outputId": "6cd3eac0-bbc3-4282-f8b7-4c15504963bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [812 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,739 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,307 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [23.8 kB]\n",
            "Get:14 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,268 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,228 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,077 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,019 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,369 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,164 kB]\n",
            "92% [19 Packages 48.8 kB/1,164 kB 4%]\u001b[0m^C\n",
            "Reading package lists... Done\n",
            "^C\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.19.0-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.7)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.25.0-py3-none-any.whl (467 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.2.2)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.11.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.19.0 trio-0.25.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "[\n",
            "    {\n",
            "        \"Recipe Name\": \"Sweet & Spicy Whipped Brie\",\n",
            "        \"Image Download Link\": \"https://www.traderjoes.com//content/dam/trjo/context-images/74953-double-cream-brie-pdp.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg\",\n",
            "        \"Description\": \"The idea of any double cream cheese is that it contains at least 60% butterfat, guaranteeing a rich and creamy result in every bite\\u2026 and we\\u2019re doubling down on the declaration that double cream cheeses are doubly delicious. Dubious? Doubtful? Don\\u2019t be. Instead, head to the cheese case at your neighborhood Trader Joe\\u2019s, pick up a wedge of Trader Joe\\u2019s Double Cream Brie, and taste for yourself. If you care to take it up a notch, break out your stand mixer for this elevated app\\u2014brie-lieve us, it\\u2019s worth the additional prep!\",\n",
            "        \"Serves\": \"Serves 2 - 4\",\n",
            "        \"Link\": \"Time 30 mins\"\n",
            "    },\n",
            "    {\n",
            "        \"Recipe Name\": \"Katsu-style Soyaki Tofu\",\n",
            "        \"Image Download Link\": \"https://www.traderjoes.com//content/dam/trjo/context-images/83089-hi-protein-org-tofu-pdp.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg\",\n",
            "        \"Description\": \"Filling and flavorful, with a magnetic, \\u201cone-more-bite\\u201d appeal courtesy of its panko-powered crunch, this vegan, katsu-inspired recipe takes Trader Joe\\u2019s Organic High Protein Super Firm Tofu to flavorsome new heights. Traditionally, katsu is served with a sticky-sweet tonkatsu sauce (Japanese-style barbecue sauce); this recipe leans on Trader Joe\\u2019s Soyaki as both a marinade and a dipping sauce for ease of prep.\",\n",
            "        \"Serves\": \"Serves 4\",\n",
            "        \"Link\": null\n",
            "    },\n",
            "    {\n",
            "        \"Recipe Name\": \"Tempeh & Cauliflower Stir Fry\",\n",
            "        \"Image Download Link\": \"https://www.traderjoes.com//content/dam/trjo/context-images/26426-soy-sauce-pdp.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg\",\n",
            "        \"Description\": \"Trader Joe\\u2019s Reduced Sodium Soy Sauce is aromatic, rich with umami, and slightly less salty than a full-sodium soy sauce. It\\u2019s made using the traditional Honjozo method, which uses a six month to two-year natural brewing and fermentation process\\u2014this means our Reduced Sodium Soy Sauce tastes authentic and is ideal for all your soy sauce needs. We\\u2019re pretty sure you won\\u2019t even notice the missing sodium! Use it as both the marinade and finishing sauce for this Organic 3 Grain Tempeh and cauliflower stir fry.\",\n",
            "        \"Serves\": \"Serves 4\",\n",
            "        \"Link\": \"Time 20 mins\"\n",
            "    },\n",
            "    {\n",
            "        \"Recipe Name\": \"Sweet & Spicy Whipped Brie\",\n",
            "        \"Image Download Link\": \"https://www.traderjoes.com//content/dam/trjo/context-images/74953-double-cream-brie-pdp.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg\",\n",
            "        \"Description\": \"The idea of any double cream cheese is that it contains at least 60% butterfat, guaranteeing a rich and creamy result in every bite\\u2026 and we\\u2019re doubling down on the declaration that double cream cheeses are doubly delicious. Dubious? Doubtful? Don\\u2019t be. Instead, head to the cheese case at your neighborhood Trader Joe\\u2019s, pick up a wedge of Trader Joe\\u2019s Double Cream Brie, and taste for yourself. If you care to take it up a notch, break out your stand mixer for this elevated app\\u2014brie-lieve us, it\\u2019s worth the additional prep!\",\n",
            "        \"Serves\": \"Serves 2 - 4\",\n",
            "        \"Link\": \"Time 30 mins\"\n",
            "    },\n",
            "    {\n",
            "        \"Recipe Name\": \"Katsu-style Soyaki Tofu\",\n",
            "        \"Image Download Link\": \"https://www.traderjoes.com//content/dam/trjo/context-images/83089-hi-protein-org-tofu-pdp.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg\",\n",
            "        \"Description\": \"Filling and flavorful, with a magnetic, \\u201cone-more-bite\\u201d appeal courtesy of its panko-powered crunch, this vegan, katsu-inspired recipe takes Trader Joe\\u2019s Organic High Protein Super Firm Tofu to flavorsome new heights. Traditionally, katsu is served with a sticky-sweet tonkatsu sauce (Japanese-style barbecue sauce); this recipe leans on Trader Joe\\u2019s Soyaki as both a marinade and a dipping sauce for ease of prep.\",\n",
            "        \"Serves\": \"Serves 4\",\n",
            "        \"Link\": null\n",
            "    },\n",
            "    {\n",
            "        \"Recipe Name\": \"Tempeh & Cauliflower Stir Fry\",\n",
            "        \"Image Download Link\": \"https://www.traderjoes.com//content/dam/trjo/context-images/26426-soy-sauce-pdp.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg\",\n",
            "        \"Description\": \"Trader Joe\\u2019s Reduced Sodium Soy Sauce is aromatic, rich with umami, and slightly less salty than a full-sodium soy sauce. It\\u2019s made using the traditional Honjozo method, which uses a six month to two-year natural brewing and fermentation process\\u2014this means our Reduced Sodium Soy Sauce tastes authentic and is ideal for all your soy sauce needs. We\\u2019re pretty sure you won\\u2019t even notice the missing sodium! Use it as both the marinade and finishing sauce for this Organic 3 Grain Tempeh and cauliflower stir fry.\",\n",
            "        \"Serves\": \"Serves 4\",\n",
            "        \"Link\": \"Time 20 mins\"\n",
            "    },\n",
            "    {\n",
            "        \"Recipe Name\": \"Sweet & Spicy Whipped Brie\",\n",
            "        \"Image Download Link\": \"https://www.traderjoes.com//content/dam/trjo/context-images/74953-double-cream-brie-pdp.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg\",\n",
            "        \"Description\": \"The idea of any double cream cheese is that it contains at least 60% butterfat, guaranteeing a rich and creamy result in every bite\\u2026 and we\\u2019re doubling down on the declaration that double cream cheeses are doubly delicious. Dubious? Doubtful? Don\\u2019t be. Instead, head to the cheese case at your neighborhood Trader Joe\\u2019s, pick up a wedge of Trader Joe\\u2019s Double Cream Brie, and taste for yourself. If you care to take it up a notch, break out your stand mixer for this elevated app\\u2014brie-lieve us, it\\u2019s worth the additional prep!\",\n",
            "        \"Serves\": \"Serves 2 - 4\",\n",
            "        \"Link\": \"Time 30 mins\"\n",
            "    },\n",
            "    {\n",
            "        \"Recipe Name\": \"Katsu-style Soyaki Tofu\",\n",
            "        \"Image Download Link\": \"https://www.traderjoes.com//content/dam/trjo/context-images/83089-hi-protein-org-tofu-pdp.jpg/jcr:content/renditions/cq5dam.web.1280.1280.jpeg\",\n",
            "        \"Description\": \"Filling and flavorful, with a magnetic, \\u201cone-more-bite\\u201d appeal courtesy of its panko-powered crunch, this vegan, katsu-inspired recipe takes Trader Joe\\u2019s Organic High Protein Super Firm Tofu to flavorsome new heights. Traditionally, katsu is served with a sticky-sweet tonkatsu sauce (Japanese-style barbecue sauce); this recipe leans on Trader Joe\\u2019s Soyaki as both a marinade and a dipping sauce for ease of prep.\",\n",
            "        \"Serves\": \"Serves 4\",\n",
            "        \"Link\": null\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Update the package lists for packages that need upgrading\n",
        "!apt update\n",
        "\n",
        "# Install the Chromium web browser and the ChromeDriver\n",
        "!apt install chromium-chromedriver\n",
        "\n",
        "# Install the Selenium package, which is used for automating web browser interaction\n",
        "!pip install selenium\n",
        "\n",
        "# Import the BeautifulSoup library for parsing HTML and XML documents\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Import the webdriver module from the Selenium package, which allows programmatic control of web browsers\n",
        "from selenium import webdriver\n",
        "\n",
        "# Create an instance of ChromeOptions to set options for the Chrome or Chromium web browser controlled by WebDriver\n",
        "options = webdriver.ChromeOptions()\n",
        "\n",
        "# Add the '--headless' argument to run the browser in headless mode, without a graphical user interface\n",
        "options.add_argument('--headless')\n",
        "\n",
        "# Add the '--no-sandbox' argument to disable the sandbox mode, which can sometimes cause issues in headless mode\n",
        "options.add_argument('--no-sandbox')\n",
        "\n",
        "# Add the '--disable-dev-shm-usage' argument to disable the use of the '/dev/shm' shared memory space in headless mode\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Initialize a new instance of the Chrome WebDriver with the specified options\n",
        "wd = webdriver.Chrome(options=options)\n",
        "\n",
        "import json\n",
        "# wd.get(\"https://example.com/\")\n",
        "def scrape_data(url):\n",
        "    wd.get(url)\n",
        "    soup = BeautifulSoup(wd.page_source, 'html.parser')\n",
        "    scraped_data = []\n",
        "    recipe_listings = soup.find_all('div', class_='MoreRecipes_list__item__2tu5X')\n",
        "\n",
        "\n",
        "    for recipe in recipe_listings:\n",
        "        recipe_name = recipe.find(class_=\"Recipe_recipe__title__2ZNSF\").text.strip()\n",
        "        image = 'https://www.traderjoes.com/' + recipe.find('img')['src'].strip()\n",
        "        description = recipe.find(class_=\"Recipe_recipe__desc__3ktf2\").find_all('p')[0].text.strip()\n",
        "        serves = recipe.find(class_=\"Recipe_recipe__complexity__Xy857\").find_all('span')[0].text.strip()\n",
        "        try:\n",
        "            time = recipe.find(class_=\"Recipe_recipe__complexity__Xy857\").find_all('span')[1].text.strip()\n",
        "        except IndexError:\n",
        "            time = None\n",
        "\n",
        "\n",
        "        # Create recipe data dictionary\n",
        "        recipe_data = {\n",
        "            \"Recipe Name\": recipe_name,\n",
        "            \"Image Download Link\": image,\n",
        "            \"Description\": description,\n",
        "            \"Serves\": serves,\n",
        "            \"Link\": time\n",
        "        }\n",
        "\n",
        "\n",
        "        # Append recipe data to scraped_data list\n",
        "        scraped_data.append(recipe_data)\n",
        "\n",
        "\n",
        "    # Validate scraped data\n",
        "    for recipe in scraped_data:\n",
        "        if not Validation.validate_mandatory_fields(recipe['Recipe Name']):\n",
        "            print(\"Validation Error: Recipe Name is mandatory but missing.\")\n",
        "            return None\n",
        "        if not Validation.validate_mandatory_fields(recipe['Image Download Link']):\n",
        "            print(\"Validation Error: Image Download Link is mandatory but missing.\")\n",
        "            return None\n",
        "        if not Validation.validate_mandatory_fields(recipe['Description']):\n",
        "            print(\"Validation Error: Description is mandatory but missing.\")\n",
        "            return None\n",
        "        if not Validation.validate_mandatory_fields(recipe['Serves']):\n",
        "            print(\"Validation Error: Serves is mandatory but missing.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        # Perform additional validation as needed\n",
        "\n",
        "\n",
        "    # Convert scraped_data to JSON format\n",
        "    json_data = json.dumps(scraped_data, indent=4)\n",
        "    return json_data\n",
        "\n",
        "\n",
        "# URL to scrape data from\n",
        "url = \"https://www.traderjoes.com\"\n",
        "\n",
        "\n",
        "# Call the function to scrape data\n",
        "print(scrape_data(url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3lR9mAjgXXZ"
      },
      "source": [
        "### https://www.lechocolat-alainducasse.com/uk/ (only UK based products)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CvTOWa8zgO_",
        "outputId": "24d8bb0c-81a5-4633-f500-a06512d9f6f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"Product Name\": \"The Blend - 75% Dark Chocolate\",\n",
            "        \"Quantity\": \"75% Dark Chocolate\",\n",
            "        \"Weight\": \"75g\",\n",
            "        \"Price\": \"\\u00a310.00\",\n",
            "        \"Link\": \"https://www.lechocolat-alainducasse.com/uk/signature-bar-dark-the-blend-75\",\n",
            "        \"Description\": \"\",\n",
            "        \"Information\": \"The Signature chocolate bars are the combination of a taste with a unique identity, discover recipes revealing the raw and pure character of our selected beans.\",\n",
            "        \"Ingredients\": \"Cocoa beans (Ecuador, Sao Tome, Madagascar, Peru), cocoa butter, sugar, emulsifier: non-GMO sunflower lecithin, sea salt. Cocoa : 75% minimum\"\n",
            "    },\n",
            "    {\n",
            "        \"Product Name\": \"Drag\\u00e9es - Hazelnut - 45% Milk Chocolate\",\n",
            "        \"Quantity\": \"45% Milk Chocolate\",\n",
            "        \"Weight\": \"from 150g\",\n",
            "        \"Price\": \"from \\u00a322.00\",\n",
            "        \"Link\": \"https://www.lechocolat-alainducasse.com/uk/hazelnut-dragee-milk#/77-size-150g\",\n",
            "        \"Description\": \"\",\n",
            "        \"Information\": \"Filling: hazelnuts, sugar, coco powder, butter, unrefined sugar. Milk chocolate case: cocoa beans, sugar, powdered milk, cocoa butter, emulsifier: sunflower lecithin non-GMO, vanilla pods, fleur de sel.\",\n",
            "        \"Ingredients\": \"For 100g : Energy 2416kJ/582kcal ; Fat: 43g (incl. Saturated fats: 13g) ; Carbohydrates: 34g (incl. saturated fats: 33g) ; Protein: 12g ; Salt: 0,18g\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "def scrape_data(url):\n",
        "    # Send a GET request to the URL\n",
        "    scraped_data = []\n",
        "    response = requests.get(url)\n",
        "\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "\n",
        "        # Example: Scraping all the product listings on the webpage\n",
        "        product_listings = soup.find_all('div', class_='productMiniature js-product-miniature')\n",
        "        for product in product_listings:\n",
        "          product_name = product['data-product-name']\n",
        "          quantity = product.find(class_ = \"productMiniature__title product-title\").find('small').text.strip()\n",
        "          weight = product.find(class_ = \"productMiniature__weight\").text.strip()\n",
        "          price = product.find(class_ = \"productMiniature__prices product-price-and-shipping\").text.strip()\n",
        "          href = product.find(class_ = \"productMiniature__name\")['href']\n",
        "          linked_page = requests.get(href)\n",
        "          linked_soup = BeautifulSoup(linked_page.content, 'html.parser')\n",
        "          linked_description = linked_soup.find(class_ = \"productDescription__text wysiwyg-content product-description\").get_text(separator=\"<br/>\").split(\"<br/>\")[0].strip()\n",
        "          linked_information = linked_soup.find(class_ = \"productAccordion__content js-tab-content\").find_all('p')[1].text.strip()\n",
        "          linked_ingredients = linked_soup.find(class_ = \"productAccordion__content js-tab-content\").find_all('p')[2].text.strip()\n",
        "            # Extract other product information here\n",
        "\n",
        "\n",
        "            # Validate the scraped data before adding to scraped_data list\n",
        "          if Validation.validate_mandatory_fields(product_name):\n",
        "                product_data = {\n",
        "                    \"Product Name\": product_name,\n",
        "                    # Add other product information to product_data\n",
        "                    \"Quantity\": quantity,\n",
        "                    \"Weight\": weight,  # Assuming weight needs to be stripped of whitespace\n",
        "                    \"Price\": price,    # Assuming price needs to be stripped of whitespace\n",
        "                    \"Link\": href,\n",
        "                    \"Description\": linked_description,\n",
        "                    \"Information\": linked_information,\n",
        "                    \"Ingredients\": linked_ingredients\n",
        "                }\n",
        "                scraped_data.append(product_data)\n",
        "          else:\n",
        "                print(f\"Skipping invalid product: {product_name}\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to retrieve data from the URL\")\n",
        "\n",
        "\n",
        "    # Convert scraped data list to JSON format\n",
        "    json_data = json.dumps(scraped_data, indent=4)\n",
        "    return json_data\n",
        "\n",
        "\n",
        "# URL to scrape data from\n",
        "url = \"https://www.lechocolat-alainducasse.com/uk/\"\n",
        "\n",
        "\n",
        "# Call the function to scrape data\n",
        "print(scrape_data(url))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueLk5lgLg1Fe"
      },
      "source": [
        "### https://foreignfortune.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PszZzXhez7oI",
        "outputId": "c69a1e29-97fb-4c3c-9465-da457bcba319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"title\": \"Toddlers/Baby\\u2019s Foreign Hoodie Jogger sets\\n\\u2013 Foreign Fortune Clothing\",\n",
            "        \"description\": \"Foreign Fortune Clothing Is A Unisex Clothing Line That Provides Top Quality Products At Affordable Prices. We Also Do Customized Outfits And Wholesale Orders. We Take Pride In Great Customer Service! We Are Located In FairLane Mall ( Dearborn, Michigan ) On The 3rd Floor Next To Jimmy Jazz. Please Check Us Out :)\",\n",
            "        \"url\": \"https://foreignfortune.com/collections/foreign-kids/products/toddlers-baby-s-foreign-hoodie-jogger-sets\",\n",
            "        \"price\": \"$60.00\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"Foreign Fortune Collection Joggers- Kid\\n\\u2013 Foreign Fortune Clothing\",\n",
            "        \"description\": \"Our Foreign Fortune Collection Joggers are great for men women and kids. They are versatile, comfortable and come in 5 colors. Great for all seasons.\",\n",
            "        \"url\": \"https://foreignfortune.com/collections/foreign-kids/products/foreign-fortune-collection-joggers\",\n",
            "        \"price\": \"$90.00\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"Kids Foreign Fortune Joggers\\n\\u2013 Foreign Fortune Clothing\",\n",
            "        \"description\": \"Foreign Fortune Clothing Is A Unisex Clothing Line That Provides Top Quality Products At Affordable Prices. We Also Do Customized Outfits And Wholesale Orders. We Take Pride In Great Customer Service! We Are Located In FairLane Mall ( Dearborn, Michigan ) On The 3rd Floor Next To Jimmy Jazz. Please Check Us Out :)\",\n",
            "        \"url\": \"https://foreignfortune.com/collections/foreign-kids/products/kids-foreign-fortune-joggers\",\n",
            "        \"price\": \"$60.00\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"Toddlers/Baby\\u2019s Plain Jain Track Suits\\n\\u2013 Foreign Fortune Clothing\",\n",
            "        \"description\": \"Foreign Fortune Clothing Is A Unisex Clothing Line That Provides Top Quality Products At Affordable Prices. We Also Do Customized Outfits And Wholesale Orders. We Take Pride In Great Customer Service! We Are Located In FairLane Mall ( Dearborn, Michigan ) On The 3rd Floor Next To Jimmy Jazz. Please Check Us Out :)\",\n",
            "        \"url\": \"https://foreignfortune.com/collections/foreign-kids/products/toddlers-baby-s-plain-jain-track-suits\",\n",
            "        \"price\": \"$60.00\"\n",
            "    },\n",
            "    {\n",
            "        \"title\": \"Toddlers/Baby\\u2019s Foreign Tour Short Sets\\n\\u2013 Foreign Fortune Clothing\",\n",
            "        \"description\": \"Foreign Fortune Clothing Is A Unisex Clothing Line That Provides Top Quality Products At Affordable Prices. We Also Do Customized Outfits And Wholesale Orders. We Take Pride In Great Customer Service! We Are Located In FairLane Mall ( Dearborn, Michigan ) On The 3rd Floor Next To Jimmy Jazz. Please Check Us Out :)\",\n",
            "        \"url\": \"https://foreignfortune.com/collections/foreign-kids/products/toddlers-baby-s-foreign-tour-short-sets\",\n",
            "        \"price\": \"$60.00\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "\n",
        "def scrape_data(url):\n",
        "    scraped_data = []  # Initialize an empty list to store the scraped data\n",
        "\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "\n",
        "        # Scraping all the links on the webpage\n",
        "        links = soup.find_all('a', class_='grid-view-item__link grid-view-item__image-container product-card__link')\n",
        "        for link in links:\n",
        "            # Extract the URL from the link\n",
        "            href = link.get('href')\n",
        "\n",
        "\n",
        "            # Check if the URL is valid\n",
        "            if href.startswith('/collections/'):\n",
        "                # Send a GET request to the linked page\n",
        "                linked_page = requests.get('https://foreignfortune.com' + href)\n",
        "\n",
        "\n",
        "                # Parse the HTML content of the linked page\n",
        "                linked_soup = BeautifulSoup(linked_page.content, 'html.parser')\n",
        "\n",
        "\n",
        "                # Scraping data from the linked page\n",
        "                linked_title = linked_soup.title.text\n",
        "                linked_description = linked_soup.find('meta', property='og:description')['content']\n",
        "                # product_id = linked_soup.find('div', id='FeaturedImageZoom-product-template-22720837189825-wrapper')['data-image-id']\n",
        "                # product_id = product_id_tag['data-image-id'] if product_id_tag else None\n",
        "                linked_price = linked_soup.find_all('span', id='ProductPrice-product-template')[0].text.strip()\n",
        "\n",
        "\n",
        "                # Validate the scraped data\n",
        "                if Validation.validate_mandatory_fields(linked_title):\n",
        "                    # Store scraped data in a dictionary\n",
        "                    product_data = {\n",
        "                        \"title\": linked_title,\n",
        "                        \"description\": linked_description,\n",
        "                        \"url\": 'https://foreignfortune.com' + href,\n",
        "                        # \"productId\": product_id,\n",
        "                        \"price\": linked_price\n",
        "                    }\n",
        "\n",
        "\n",
        "                    scraped_data.append(product_data)\n",
        "                else:\n",
        "                    print(\"Skipping invalid data:\", linked_title)\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to retrieve data from the URL\")\n",
        "\n",
        "\n",
        "    # Convert scraped data list to JSON format\n",
        "    json_data = json.dumps(scraped_data, indent=4)\n",
        "\n",
        "\n",
        "    return json_data\n",
        "\n",
        "\n",
        "# URL to scrape data from\n",
        "url = \"https://foreignfortune.com/collections/foreign-kids\"\n",
        "\n",
        "\n",
        "# Call the function to scrape data and print the JSON output\n",
        "scraped_json_data = scrape_data(url)\n",
        "print(scraped_json_data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}